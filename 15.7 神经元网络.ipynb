{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "RBF神经网络经过这样两层变化: $\\left\\{\\begin{array}{l}R_i(X)=exp(-||X-C_i||^2/2\\sigma_i^2),\\qquad i=1,\\cdots,m\\\\\\hat{y}_k=\\sum\\limits_{i=1}^{m}\\omega_{ik}R_i(X),\\qquad k=1,\\cdots,p\\end{array}\\right.$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "这样只有小部分靠近中心的隐藏层神经元被激活($R_i(X)$随着其中范数增大,指数减少)"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "确定基函数中心$C_i$. 一般采用K均值聚类法."
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "确定基函数宽度$\\sigma_i$. 通常令它等于基函数中心与子样本集中样本模式之间的平均距离"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "确定权值$\\omega_{ik}$. 采用最小均方误差测度."
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1.BP神经网络\n",
    "#   见 'MPLClassifier.ipynb'\n",
    "# 2.RBF神经网络\n",
    "from IPython.display import Latex\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "\n",
    "def print_latex(latex_str):\n",
    "    display(Latex(latex_str))\n",
    "    \n",
    "    \n",
    "print_latex(r'RBF神经网络经过这样两层变化: $\\left\\{\\begin{array}{l}R_i(X)=exp(-||X-C_i||^2/2\\sigma_i^2),\\qquad i=1,\\cdots,m\\\\\\hat{y}_k=\\sum\\limits_{i=1}^{m}\\omega_{ik}R_i(X),\\qquad k=1,\\cdots,p\\end{array}\\right.$')\n",
    "print_latex(r'这样只有小部分靠近中心的隐藏层神经元被激活($R_i(X)$随着其中范数增大,指数减少)')\n",
    "print_latex(r'确定基函数中心$C_i$. 一般采用K均值聚类法.')\n",
    "print_latex(r'确定基函数宽度$\\sigma_i$. 通常令它等于基函数中心与子样本集中样本模式之间的平均距离')\n",
    "print_latex(r'确定权值$\\omega_{ik}$. 采用最小均方误差测度.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function at step 0 is 0.12536238133907318\n",
      "Loss function at step 20 is 0.04855727404356003\n",
      "Loss function at step 40 is 0.0013868508394807577\n",
      "Loss function at step 60 is 0.007514643482863903\n",
      "Loss function at step 80 is 0.00722929323092103\n",
      "Loss function at step 100 is 0.002718414645642042\n",
      "Loss function at step 120 is 0.06919346004724503\n",
      "Loss function at step 140 is 0.06208699941635132\n",
      "Loss function at step 160 is 0.06197843328118324\n",
      "Loss function at step 180 is 0.06126131862401962\n",
      "Training complete!\n",
      "Relative Error Sum: 1580.5765%\n",
      "                                                Input Pred Out Raw Out  \\\n",
      "0   [-0.76797444 -0.13832668 -0.38830596  1.31866573]  -0.0287    -0.2   \n",
      "1   [ 0.02401949 -0.32662479 -0.67466729 -0.59120002]  -0.0518    -0.1   \n",
      "2   [ 0.50440925 -0.51492289  3.57493482 -1.02038333]   1.9200     1.9   \n",
      "3   [-1.11852913 -0.45698501 -0.38830596 -0.62338876]  -0.0514    -0.3   \n",
      "4   [ 0.10841228  3.85938689 -0.60594057  1.15772199]  -2.5421    -2.6   \n",
      "5   [-1.11852913  0.73074149 -0.53721385  1.44742073]  -0.0436     0.8   \n",
      "6   [ 0.51739275 -0.35559373 -0.64030393 -0.6663071 ]  -0.0519     0.3   \n",
      "7   [-1.21590543 -0.44250054  0.06987216 -0.88089875]  -0.0506    -0.5   \n",
      "8   [-0.1317826  -0.84806569  1.75367678  2.38089444]   0.5512     0.4   \n",
      "9   [-0.7874497  -0.08038881 -0.56012276  0.4602991 ]  -0.0477     2.0   \n",
      "10  [-0.09932383 -0.77564334  0.24168896 -1.06330166]  -0.0467    -0.2   \n",
      "11  [ 2.39350951 -0.4135316  -0.26230698 -0.90235792]  -0.0521    -0.6   \n",
      "12  [ 1.84820222 -0.52940736 -0.75484846 -0.85943959]  -0.0523    -0.6   \n",
      "13  [-0.5083043   0.57141232  0.43641466 -0.31223086]  -0.0469     0.7   \n",
      "14  [ 0.53037626 -0.60182971 -0.03321791  1.22209949]  -0.0280    -0.2   \n",
      "15  [-1.07308685  0.41208316 -0.6975762  -0.65557751]  -0.0522    -0.8   \n",
      "16  [-0.17722487  0.23826952  0.09278107  0.12768204]  -0.0467    -0.0   \n",
      "17  [-0.31355169  0.67280361  0.34477904  0.56759493]  -0.0428    -1.0   \n",
      "18  [-0.38496098 -0.52940736 -0.60594057 -0.93454667]  -0.0520    -0.3   \n",
      "19  [ 1.77030118 -0.47146948 -0.36539706 -0.17274628]  -0.0462     1.3   \n",
      "\n",
      "      Error Relative Error  \n",
      "0    0.1277      -81.6335%  \n",
      "1    0.0312      -37.5469%  \n",
      "2    0.0342        1.8149%  \n",
      "3    0.2373      -82.2016%  \n",
      "4    0.0387       -1.5003%  \n",
      "5   -0.8421     -105.4561%  \n",
      "6   -0.3803     -115.8097%  \n",
      "7    0.3997      -88.7579%  \n",
      "8    0.1787       47.9808%  \n",
      "9   -2.0070     -102.4360%  \n",
      "10   0.1245      -72.7059%  \n",
      "11   0.5598      -91.4843%  \n",
      "12   0.5890      -91.8433%  \n",
      "13  -0.7132     -107.0365%  \n",
      "14   0.1432      -83.6460%  \n",
      "15   0.7949      -93.8414%  \n",
      "16  -0.0225       92.8149%  \n",
      "17   0.9365      -95.6320%  \n",
      "18   0.2515      -82.8758%  \n",
      "19  -1.3443     -103.5589%  \n"
     ]
    }
   ],
   "source": [
    "# RBF神经网络(based on tensorflow): https://github.com/shiluqiang/RBF_NN_tensorflow/blob/master/RBF_tensorflow.py\n",
    "# 由于数据量小, 本人的不熟悉等原因, 预测结果并不理想.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "class RBF_NN():\n",
    "    def __init__(self, hidden_nodes, input_data_trainX, input_data_trainY):\n",
    "        self.hidden_nodes = hidden_nodes #隐含层节点数\n",
    "        self.input_data_trainX = input_data_trainX #训练样本的特征\n",
    "        self.input_data_trainY = input_data_trainY #训练样本的标签\n",
    "    \n",
    "    def fit(self):\n",
    "        '''模型训练\n",
    "        '''\n",
    "        # 1.声明输入输出的占位符\n",
    "        n = 19\n",
    "        n_input = (self.input_data_trainX).shape[1]\n",
    "        n_output = (self.input_data_trainY).shape[0]\n",
    "        X = tf.placeholder('float', [None, n_input],name = 'X')\n",
    "        Y = tf.placeholder('float', [None, 1],name = 'Y')\n",
    "        \n",
    "        # 2.参数设置\n",
    "        ## RBF函数参数\n",
    "        ### K-means求中心\n",
    "        random_state = 170\n",
    "        kms = KMeans(n_clusters=self.hidden_nodes, random_state=None)\n",
    "        pred = kms.fit_predict(trainX)\n",
    "        \n",
    "        # c = tf.Variable(tf.random_normal(shape=(self.hidden_nodes, n_input)),name = 'c')\n",
    "        # c = tf.concat((tf.cast(tf.Variable(kms.cluster_centers_), tf.float32), c), axis=0)\n",
    "        c = tf.cast(tf.Variable(kms.cluster_centers_), tf.float32)\n",
    "        delta = tf.Variable(tf.random_normal(shape=(1,self.hidden_nodes)), name='delta')\n",
    "        ## 隐含层到输出层权重和偏置\n",
    "        W = tf.Variable(tf.random_normal(shape=(self.hidden_nodes, 1)), name='W')\n",
    "        b = tf.Variable(tf.random_normal(shape=(1, 1)), name='b')\n",
    "        \n",
    "        # 3.构造前向传播计算图\n",
    "        ## 隐含层输出\n",
    "        ### 特征样本与RBF均值的距离\n",
    "        dist = tf.reduce_sum(tf.square(tf.subtract(tf.tile(X,[self.hidden_nodes, 1]),c)), axis=1)\n",
    "        dist = tf.multiply(1.0,tf.transpose(dist))\n",
    "        ### RBF方差的平方\n",
    "        delta_2 = tf.square(delta)\n",
    "        ### 隐含层输出\n",
    "        RBF_OUT = tf.exp(tf.multiply(-1.0,tf.divide(dist,tf.multiply(2.0,delta_2))))\n",
    "        ## 输出层输入\n",
    "        output_in = tf.matmul(RBF_OUT, W) + b\n",
    "        \n",
    "        # 4.声明代价函数优化算法\n",
    "        loss = tf.reduce_mean(tf.pow(Y - output_in,2)) #损失函数为均方误差\n",
    "        train_op = tf.train.AdamOptimizer(0.05).minimize(loss) #优化算法为梯度下降法\n",
    "        \n",
    "        # 5.反向传播求参数\n",
    "        trX = self.input_data_trainX[:n]\n",
    "        trY = self.input_data_trainY[:n]\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            ## 初始化所有参数\n",
    "            tf.global_variables_initializer().run()\n",
    "            for epoch in range(200):\n",
    "                for i in range(trX.shape[0]):\n",
    "                    feed = {X:trX[i][:,None].T, Y:[[trY[i]]]}\n",
    "                    sess.run(train_op,feed_dict=feed)\n",
    "                if epoch % 20. == 0 :\n",
    "                    total_loss = 0.0\n",
    "                    for j in range(trX.shape[0]):\n",
    "                        total_loss += sess.run(loss, feed_dict={X:trX[i][:,None].T, Y:[[trY[i]]]})\n",
    "                    print('Loss function at step %d is %s'%(epoch, total_loss / trX.shape[0]))\n",
    "                    \n",
    "            print('Training complete!')\n",
    "\n",
    "            W = W.eval()\n",
    "            b = b.eval()\n",
    "            c = c.eval()\n",
    "            delta = delta.eval()\n",
    "            pred_trX = np.mat(np.zeros((len(trX),n_output)))\n",
    "            \n",
    "            ## 训练准确率\n",
    "            correct_tr = 0.0\n",
    "            pred = []\n",
    "            for i in range(self.input_data_trainX.shape[0]):\n",
    "                pred_tr = sess.run(output_in, feed_dict={X:self.input_data_trainX[i][:,None].T})\n",
    "                pred.append(pred_tr[0][0])\n",
    "            df_columns = ['Input', 'Pred Out', 'Raw Out', 'Error', 'Relative Error']\n",
    "            pred = np.array(pred)\n",
    "            df = pd.DataFrame(np.c_[[x.__str__() for x in self.input_data_trainX],\n",
    "                                    pred,\n",
    "                                    self.input_data_trainY,\n",
    "                                    np.subtract(pred, self.input_data_trainY),\n",
    "                                    np.array(np.divide(np.subtract(pred, self.input_data_trainY), self.input_data_trainY), dtype=np.float)\n",
    "                                   ], columns=df_columns)\n",
    "            print('Relative Error Sum: {:.4%}'.format(df['Relative Error'].astype(float).abs().sum()))\n",
    "            df['Pred Out'] = df['Pred Out'].apply(lambda x: format(float(x), '.4f'))\n",
    "            df['Raw Out'] = df['Raw Out'].apply(lambda x: format(float(x), '.1f'))\n",
    "            df['Error'] = df['Error'].apply(lambda x: format(float(x), '.4f'))\n",
    "            df['Relative Error'] = df['Relative Error'].apply(lambda x: format(float(x), '.4%'))\n",
    "            print(df)\n",
    "            # self.save_model('RBF_predict_results.txt',pred_trX)\n",
    "            \n",
    "    def save_model(self,file_name,weights):\n",
    "        '''保存模型(保存权重weights)\n",
    "        input：file_name(string):文件名\n",
    "               weights(mat)：权重矩阵\n",
    "        '''\n",
    "        f_w = open(file_name,'w')\n",
    "        m,n = np.shape(weights)\n",
    "        for i in range(m):\n",
    "            w_tmp = []\n",
    "            for j in range(n):\n",
    "                w_tmp.append(str(weights[i,j]))\n",
    "            f_w.write('\\t'.join(w_tmp)+'\\n')\n",
    "        f_w.close()\n",
    "            \n",
    "\n",
    "data = np.loadtxt('15.D 水库年径流与因子特征.txt')\n",
    "data = preprocessing.scale(data)\n",
    "\n",
    "trainX = data[:, :-1]\n",
    "trainY = data[:, -1]\n",
    "input_data_trainX = trainX\n",
    "input_data_trainY = trainY\n",
    "rbf = RBF_NN(10, input_data_trainX, input_data_trainY)\n",
    "rbf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the end of epoch: 0, with loss: 570.4130045572916\n",
      "At the end of epoch: 200, with loss: 66.69202931722005\n",
      "At the end of epoch: 400, with loss: 54.4934336344401\n",
      "At the end of epoch: 600, with loss: 50.84969584147135\n",
      "At the end of epoch: 800, with loss: 47.14697392781576\n",
      "Relative Error Sum: 1167.3581%\n",
      "                    Input Pred Out Raw Out     Error Relative Error\n",
      "0   [15.6  5.6  3.5 25.5]  26.6027    22.9    3.7027       16.1688%\n",
      "1   [27.8  4.3  1.   7.7]  20.5351    23.4   -2.8649      -12.2434%\n",
      "2   [35.2  3.  38.1  3.7]  34.0356    36.8   -2.7644       -7.5120%\n",
      "3   [10.2  3.4  3.5  7.4]  14.9736    22.0   -7.0264      -31.9380%\n",
      "4   [29.1 33.2  1.6 24. ]  53.6574     6.4   47.2574      738.3962%\n",
      "5   [10.2 11.6  2.2 26.7]  31.0360    29.4    1.6360        5.5647%\n",
      "6   [35.4  4.1  1.3  7. ]  21.6147    26.2   -4.5853      -17.5013%\n",
      "7       [8.7 3.5 7.5 5. ]  12.7028    20.9   -8.1972      -39.2212%\n",
      "8   [25.4  0.7 22.2 35.4]  27.7008    26.5    1.2008        4.5311%\n",
      "9   [15.3  6.   2.  17.5]  24.1472    37.3  -13.1528      -35.2623%\n",
      "10  [25.9  1.2  9.   3.3]  16.0947    22.8   -6.7053      -29.4093%\n",
      "11  [64.3  3.7  4.6  4.8]  25.8643    19.8    6.0643       30.6276%\n",
      "12  [55.9  2.9  0.3  5.2]  24.2646    19.6    4.6646       23.7991%\n",
      "13  [19.6 10.5 10.7 10.3]  24.4124    28.5   -4.0876      -14.3424%\n",
      "14  [35.6  2.4  6.6 24.6]  27.4311    22.8    4.6311       20.3119%\n",
      "15  [10.9  9.4  0.8  7.1]  20.5961    18.2    2.3961       13.1656%\n",
      "16  [24.7  8.2  7.7 14.4]  25.8062    23.8    2.0062        8.4296%\n",
      "17  [22.6 11.2  9.9 18.5]  30.0362    17.3   12.7362       73.6199%\n",
      "18  [21.5  2.9  1.6  4.5]  15.9504    21.9   -5.9496      -27.1672%\n",
      "19  [54.7  3.3  3.7 11.6]  26.8479    32.8   -5.9521      -18.1466%\n"
     ]
    }
   ],
   "source": [
    "# BP神经网络(based on keras)\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "data = np.loadtxt('15.D 水库年径流与因子特征.txt')\n",
    "X = data[:, :-1]\n",
    "Y = data[:, -1]\n",
    "n = 5\n",
    "trainX = X[n:]\n",
    "trainY = Y[n:]\n",
    "\n",
    "model = Sequential()  #层次模型\n",
    "model.add(Dense(12, input_dim=4)) #输入层，Dense表示BP层\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(5, input_dim=12))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, input_dim=5))  #输出层\n",
    "model.compile(loss='mean_squared_error', optimizer='Adam') #编译模型\n",
    "\n",
    "\n",
    "def eposh_callback(epoch, logs):\n",
    "    if epoch % 200 == 0:\n",
    "        print('At the end of epoch: {}, with loss: {}'.format(epoch, logs['loss']))\n",
    "        \n",
    "        \n",
    "batch_print_callback = LambdaCallback(on_epoch_end=eposh_callback)\n",
    "model.fit(trainX, trainY, epochs = 1000, batch_size = 5, verbose=0, callbacks=[batch_print_callback]) #训练模型1000次\n",
    "\n",
    "pred = np.array(model.predict(X)).flatten()\n",
    "df_columns = ['Input', 'Pred Out', 'Raw Out', 'Error', 'Relative Error']\n",
    "df = pd.DataFrame(np.c_[[x.__str__() for x in X],\n",
    "                    pred,\n",
    "                    Y,\n",
    "                    np.subtract(pred, Y),\n",
    "                    np.array(np.divide(np.subtract(pred, Y), Y), dtype=np.float)\n",
    "                   ], columns=df_columns)\n",
    "print('Relative Error Sum: {:.4%}'.format(df['Relative Error'].astype(float).abs().sum()))\n",
    "df['Pred Out'] = df['Pred Out'].apply(lambda x: format(float(x), '.4f'))\n",
    "df['Raw Out'] = df['Raw Out'].apply(lambda x: format(float(x), '.1f'))\n",
    "df['Error'] = df['Error'].apply(lambda x: format(float(x), '.4f'))\n",
    "df['Relative Error'] = df['Relative Error'].apply(lambda x: format(float(x), '.4%'))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
